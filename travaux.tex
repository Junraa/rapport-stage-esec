\chapter{Cahier des charges}
\section*{Chronogramme}
\begin{center}
\end{center}
\section{Remise en contexte}
\myparagraph{Les vulnérabilités de type Use-After-Free}
\subparagraph{}
La plupart des vulnérabilités enseignées et étudiées dans le cadre scolaire
se rapportent à des fonctionnement souvent statique, ou du moins dont le comportement
est uniquement lié au programme ciblé en lui-même (à son architecture de compilation et/à
son code source). On prendra comme exemple les dépassements de tampons, les problèmes de
formatage des chaines de caractères, etc.\subparagraph{}
Dans l'environnement industriel, et sur les solutions importantes utilisées dans le monde
de l'entreprise, ces vulnérabilités sont de plus en plus rare. Premièrement parce que le temps
faisant son œuvre, la grande majorité ont été reportées sur les solutions les plus anciennes,
mais également parce que de nombreux outils pouvant les détecter rapidement (parfois dès la compilation)
ont été conçus.\subparagraph{}
Les attaquants explorent donc de nouvelles voies, notamment sur des problématiques plus dynamiques, et/ou
lié également à l'environnement de production du programme, comme le système d'exploitation ou la bibliothèque utilisée.
En effet, de nombreuses opérations du programme dépendent intrinsèquement du système d'exploitation sous-jacent et
même si le comportement final reste souvent le même sur les différentes plateformes, les effets collatéraux
sur le contexte et le mécanisme d'exécution est souvent différent.\subparagraph{}

\subparagraph{L'évolution des vulnérabilités de type use-after-free}
\includegraphics[scale=0.5]{histogramme-uaf.png}\newline
Beaucoup de problème de corruption mémoire sont directement dû à la mauvaise utilisation
du langage utilisé pour le développement. En effet, les langages de programmation tels que C
ou C++ demandent une gestion de la mémoire manuelle (le C++ permet néanmoins une gestion plus ou moins automatisée
avec les nouveaux standards).
\subparagraph{}
Il est donc fréquent de rencontrer des erreurs sur ce point là. Cela arrive
notamment lorsque par un problème de maintenabilité ou de mauvaise conception, la partie du programme devant
libérer une zone mémoire est méconnue/non documentée.\subparagraph{}
Ce n'est cependant pas toujours le cas et les allocateurs de mémoires / ramasse-miettes sont parfois en cause.

\subparagraph{Exemple}
Un exemple très simple et souvent rencontré dans la littérature est le suivant:
\begin {lstlisting}[frame=single]
int main(void)
{
    int *a = malloc(sizeof(int));
    *a = 5;
    free(a);
    int *b = malloc(sizeof(int));
    printf("\%d", *b); // affiche 5
    *b = 7;
    printf("\%d", *b); // affiche 7
    *a = 10;
    printf("\%d", *b); // affiche 10
}
\end{lstlisting}
\subparagraph{}
Sur une distribution Linux classique comme Ubuntu, quelque soit la taille de mots
de l'architecture (32 ou 64 bits), les adresses a et b sont exactement les mêmes.
Cela est directement dû à l'algorithme d'allocation mémoire de la bibliothèque standard
C.\subparagraph{}
En effet lors du premier appel à la fonction \textbf{malloc()}, une première zone est allouée.
Lors de sa libération, la zone libérée est mise en cache, et lors du second appel à \textbf{malloc()},
l'algorithme va retourner la même zone mémoire. Ce comportement ne sera pas détaillée dans cette section, car
il demanderait à lui seul plusieurs pages.\subparagraph{}
Cela va de soi que cet exemple est simpliste, et que les vulnérabilités rencontrées en production sont
rarement aussi visible. Il permet toutefois de bien visualiser l'origine du problème.


\myparagraph{Outils existants en dynamique et en statique}
Des outils simplistes ne suffisent pas à détecter la présence d'use-after-free.\newline
Par exemple, si une solution détecte uniquement l'accès à une zone mémoire non allouée, l'exemple
donnée précédemment ne sera pas considéré comme vulnérable, car la zone mémoire est là même pour deux
allocations différentes. Plusieurs possibilités s'ouvrent alors : l'analyse statique, l'analyse dynamique,
et un mélange des deux.

\myparagraph{Analyse dynamique}
De nombreuses personnes / équipes se sont penchées sur la détection de ces comportements, dynamiquement et
statiquement. Nous parlerons tout d'abord des solutions dynamiques existantes, puis nous aborderons \textbf{les approches
statiques, qui sont directement liées au sujet de ce stage.}

\subparagraph{Kasan: Kernel Address Sanitizer}
Kasan est un projet open-source, développée majoritairement
par les acteurs du noyaux Linux ainsi que par Google. Son but est d'ajouter une couche de protection
mémoire au noyau Linux afin de détecter certains corruptions mémoires dans les opérations du noyau, comme
des use-after-free ou des accès hors limites.
https://github.com/google/kasan

\subparagraph{AddressSanitizer}
AddressSanitizer est également un projet open-source, et effectue
un travail similaire à Kernel Adress Sanitizer. Intégré dans la base de code de
LLVM, il permet, lorsque le programme testé est compilé avec les bonnes options du compilateur \textbf{Clang},
de détecter des problèmes de mémoires.

\subparagraph{Undangle}
Undangle est également un exemple connu, spécialement dédié à la découverte de vulnérabilité
de type use-after-free et double-free, à partir d'une trace d'exécution. Il a été développé par plusieurs
personnes travaillant à l'IMDEA Software Institute.

\myparagraph{Analyse statique}

De nombreux outils dynamique détectant les problèmes liés à la mémoire existent et remplissent parfaitement leur rôle.
L'analyse dynamique a néanmoins plusieurs défauts.\newline

\subparagraph{Les avantages d'une analyse statique}

Lors de la phase de test d'une application, ainsi que lors de la recherche de vulnérabilités dans celle-ci, une bonne méthodologie
est d'essayer de parcourir tous les chemins possibles lors de l'exécution. Cela nécessite donc plusieurs répétitions, ainsi qu'une connaissance
pointue du programme en lui même.\newline
Cependant, il est théoriquement impossible de parcourir tous les options possibles d'un programme, notamment si
celui-ci dépend d'entrées utilisateurs ou de paramètre du système sur lequel il opère. Cela est d'autant plus vrai lorsque le binaire atteint une taille
considérable. De plus, les bonnes suite de tests se font extrêmement rare au niveau professionnel.

\subparagraph{Les inconvénients d'une analyse statique}

Malgré le fait qu'une analyse statique autorise théoriquement le parcours de tous les chemins d'exécution existant, plusieurs obstacles
limite parfois son utilité.\newline

Les corruptions mémoires sont par nature dynamiques. Lorsqu'une allocation s'effectue, son emplacement n'est pas prédéterminé
et plusieurs facteurs extérieurs peuvent venir gêner l'opération (absence d'espace mémoire). De plus, une allocation
est essentiellement liée à sa taille, et le dépassement de tampons et autres vulnérabilités se basent souvent sur cette information.
Lors d'une analyse statique, cette taille n'est pas forcement connue, à part dans quelques cas précis.
\subparagraph{}

En dehors des contraintes sur la précision de l'analyse, une des limitations de l'analyse statique repose sur le système et l'algorithme
dévoué à l'analyse. Prendre plusieurs chemins et conserver les états, répertorier l'ensemble de ces informations en mémoire, l'analyse
statique de boucles, et autres, peuvent prendre une taille considérable en mémoire, et souvent, un compromis doit se faire entre la vitesse
d'analyse et la mémoire allouée au programme. Sur certains binaires, l'analyse peut durer plusieurs jours. Durant les tests réalisés pendant le
stage, les douze giga de mémoire étaient parfois atteints.

\subparagraph{}
Lorsqu'un binaire ou sa trace d'execution possible est analysé, surtout lorsque le traitement est général et sans destination précise (c'est à dire
qu'aucune instruction particulière dans le binaire n'est visée). On se retrouve alors face au problème d'"explosion de chemins (path explosion).

\subparagraph{}
Ces problématiques d'espace, d'analyse de boucles et d'optimisation de temps ont occupé une grande partie de ce stage et seront développées plus
en détails lors des prochaines sections de ce rapport.

\subparagraph{Les outils statiques existants} sont plutôt rare comparés à la masse d'analyseur dynamiques. Néanmoins certains sortent du lot.

\subparagraph{GUEB}
GUEB a été développé principalement par Feist Josselin durant sa thèse. Il se base sur une représentation intermédiaire
REIL fournie par l'outil BinNavi. En se basant sur cette représentation, l'outil parcours le binaire pour retrouver des vulnérabilités
de type use-after-free possibles.

\section{Présentation globale du projet}
\myparagraph{Un analyseur statique}

Le projet de ce stage fut de développer en Python2.7 un analyseur statique de fichier
binaire exécutable afin de détecter des vulnérabilités de type use-after-free sans aide
provenant de rapports fournis par des outils externes.

\myparagraph{Ambition}

Comme évoquée dans la section précédente de ce rapport, une analyse statique complète est souvent
longue et très demandante en ressources. Les connaissances en début de stage sur le sujet étant limitées,
une phase d'apprentissage étant indispensable (retirant donc du temps de développement), l'ambition du stage
a donc été revue en conséquence.
\subparagraph
Plusieurs objectifs étaient fixés concernant la globalité du projet:
\begin{itemize}
        \item Une montée en compétence sur le sujet
        \item Un outil fonctionnel même si long à l'analyse
        \item Une base maintenable, documentée et reprenable dans des travaux futurs
\end{itemize}

Il était attendu qu'à la fin du stage, ces points la soient acquis. Le côte "fonctionnel"
de l'outil était cependant laissé volontairement vague. Les fonctionnalités mises en place
devaient exister, en étant aussi performante que possible, mais le développement devait se faire
pas à pas, et la qualité d'une fonctionnalité primait sur l'accumulation de plusieurs autres.

\section{Résultat à obtenir}
\myparagraph{Objectifs d'utilisation de l'outil}

\subparagraph{Différence par rapport à l'existant}

Quasiment toutes les solutions existantes se basent sur un rapport ou une représentation agrémentée d'informations fournis
par un outil extérieur, effectuant déjà une première analyse, tel qu'IDA ou BinNavi. Un des premiers objectifs étaient donc de
se baser uniquement sur une aide externe pouvant fournir le \textbf{désassemblage} du binaire ainsi qu'une \textbf{representation intermédiaire}
pour plus de généricité au regard de l'architecture ciblée par le binaire.

\subparagraph{Format(s) visé(s)}

Les deux formats de fichier les plus courants en entreprises, PE et ELF étaient la priorité principale. Le format mach-o pour
Mac OS X n'était pas envisagé, ne serait ce qu'a cause du peu de désassembleurs supportant ce format. La différence de traitement
entre les formats d'exécutables réside quasi uniquement dans la récupération des symboles et de la table d'import. En effet, ceux-ci
sont stockés différemment selon le format, et sont indispensables pour garder la trace des fonctions allouant ou libérant de la mémoire.

\subparagraph{Architecture(s) visé(s)}
Les architectures que l'ont peu trouver sur le marché sont nombreuses et correspondent souvent à un usage bien spécifique.
Les ordinateurs de bureau et ordinateurs portables sont majoritairement équipés d'architecture x86-(64), et on retrouve de plus en plus
cet architecture sur certains serveurs, grâce à leur performance et leur prix. Les téléphones portables préfèrent souvent quand à eux
des architectures comme ARM qui consomment moins d'énergie. Ces deux architectures été initialement choisis pour l'analyseur, mais seul
le fonctionnement sur architecture x86 a été testé. Néanmoins, le support pour ARM est assuré par une abstraction par rapport aux systèmes
sous-jacents.
\subparagraph{}
La différence entre deux architectures se fait surtout sur les conventions d'appels qui peuvent les accompagner, comme le registre accueillant
la valeur de retour d'une fonction, ou les registres contrôlant la pile. Le reste des registres sont abstraits et sont de toute manière
juste traités comme des identifiants lors de l'analyse.


\chapter{Compte-rendu d'activité}
\section{Compréhension des allocateurs mémoire}
Lors de son execution, un programme est chargé en mémoire et le système d'exploitation
met à sa disposition un espace mémoire (que l'on parle de mémoire virtuelle ou physique)
d'une certaine taille. Cet espace est découpé en plusieurs parties : les segments du programme
(segment de code, segment de données, segment de pile, segment de tas), son tas et sa pile. Si les segment de code et de
données et garde généralement la même taille pour le temps de l'execution, les segments de pile et de tas sont par définition
dynamique et peuvent s'élargir ou rétrécir. Même s'il est possible d'augmenter la taille de la pile de manière dynamique (grâce
à la fonction \textbf{alloca()} sur des systèmes UNIX par exemple), son utilisation n'est pas encouragé pour stocker des données
dynamiques. Le tas, quand à lui, est un espace mémoire spécialement réservé pour cet emploi.
\begin{center}
\includegraphics[scale=0.4]{memory-layout.png}\newline
\end{center}


\myparagraph{Allocation et Libération: base des algorithmes}
\subparagraph{Allocation sur le tas}
Une des fonctions les plus connues des programmeurs et ingénieurs sous environnement UNIX est \textbf{malloc()} (respectivement \textbf{HeapAlloc()} sur
environnement Windows). En effet, en espace utilisateur, cette fonction est la base de l'allocation mémoire dynamique. Comme beaucoup de fonctions
manipulant le système d'exploitation et l'architecture sous-jacente, \textbf{malloc()} conserve son état tout au long de l'execution et répertorie les
changements dans des données réservés à son utilisation (première zone libre, pointeurs sur une structure de base, etc...).
\subparagraph{}
Il existe de nombreuses alternatives au classique \textbf{malloc()}, provenant de bibliothèque externe et permettant de s'adapter aux besoins, voir d'améliorer les
performance globales (\textbf{gmalloc()}, \textbf{TCmalloc()}. Néanmoins, la base de l'algorithme reste le même quelque soit la bibliothèque utilisée.

\subparagraph{Découpage en bloc du tas}
Pour gérer les allocations de différentes tailles, l'allocateur mémoire découpe au fur et à mesure des demandes
le segment qui lui est réservé. Ce découpage donne formation à des blocs mémoires qui sont marqués comme libre ou alloué, en gardant leur taille respective
en information.
\subparagraph{}
Pour plus de rapidité lors de l'allocation et la libération, des liens sont conservés entre les différents blocs libres (suivant et précédent). Ainsi lors d'une allocation,
un parcours simple à partir du premier bloc libre permet de trouver un espace convenant à la demande. D'autres informations peuvent être stockées selon l'implémentation.
Un des exemples le plus courant est le suivant\footnote{http://gee.cs.oswego.edu/dl/html/malloc.html}:
\begin{center}
\includegraphics[scale=0.5]{malloc.png}\newline
\end{center}

On remarque l'optimisation de l'espace pour les blocs libres. Au lieu d'utiliser de l'espace dans le cas général, qui ne servirait pas dans le cas des blocs alloués,
on préfère placer les pointeurs vers les autres blocs libres à l'intérieur de l'espace libre. Cela amenera d'ailleurs une discussion sur la granularité des allocations dans
les prochains paragraphes.
\subparagraph{}
La taille est également stockée au début et à la fin d'un bloc. Cela permet une coalescion plus simple des blocs libres. En effet quand deux blocs contigus en
mémoire sont libres, il est possible des les réunir en un seul plus gros blocs.

\subparagraph{Optimisation : cache}
Cette algorithme a cependant des limites. Si la fréquence d'allocation est rapide, et si les tailles allouées/libérés sont très variées, on retrouve un phénomene de
fragmentation important, et les blocs libres restant peuvent être trop petit pour une prochaine allocation, même s'ils auraient pu collectivement suffire.
\subparagraph{}
Pour assurer une fragmentation moins importante, la plupart ont mis au point un système de cache (look-aside lists puis Low Fragmentation Heap sur Windows, différents sur Linux).
Le principe est simple : l'allocateur garde une liste des blocs libres d'une certaines tailles. Selon l'algorithme utilisé, les tailles pré-alloués peuvent différées.
On peut découper ces stratégies de caches en deux: la refonte différé et la pré-allocation.

\subparagraph{Refonte différé}
Au lieu de faire une refonte des blocs contigus lors de la libération, les blocs sont conservés libres et indépendent, et rajouté à la liste des blocs libres de
la taille correspondante. Cela dans l'espoir que les prochaines allocations utiliseront une taille proche.

\subparagraph{Pré-Allocation}
Plutôt que d'attendre une libération, une partie de la mémoire initiale est pré-découpée pour remplir les caches. L'autre partie est conservé pour les allocations
ne rentrant pas dans les catégories mises en cache.

\subparagraph{}
Ce principe de cache est notamment très utile dans les environnements où la même taille est alloué un très grand nombre de fois. On peut imaginer par exemple un
algorithme allouant une structure pour chacun des utilisateurs s'enregistrant sur une application, ou simplement un service allouant la même taille un très grand nombre
de fois pour y stocker des messages. En terme de statistique, un programme a souvent un spectre de taille d'allocation réduit, ce qui a conduit à la présence systèmatique de
cache dans les algorithmes d'allocation mémoire.


\subparagraph{Granularité des allocations}
Comme évoqué précedemment, la granularité des blocs alloués peut varier selon les méthodes et les allocateurs utilisés. La "granularité" d'une allocation
correspond à une taille fixe dont la taille d'une allocation sera un multiple. En effet, même si l'utilisateur demande un bloc de 4 octets par exemple, la plupart
des allocateurs renverront un pointeur vers une zone allouée plus grande. Cela vient de plusieurs facteurs.

\subparagraph{}
Le premier concerne les méta-données conservées à l'intérieur ou autour du bloc alloué. Comme vu dans le schéma sur le découpage classique d'un bloc mémoire,
on retrouve plusieurs informations (comme des pointeurs en cas de blocs libres) à l'intérieur de la zone mémoire selon son état. Sur une architecture 64 bits,
deux pointeurs represente en tout 16 octets. Ainsi, sans compter les méta-donnés entourant la zone mémoire réservée à l'utilisateur, une demande de blocs de taille
inférieure à 16 octets sera tout simplement arrondie...
\subparagraph{}
Sur Windows ou UNIX, les appels aux allocateurs courant sont souvent reliés intriséquement aux fonctions d'allocation plus "générale" comme  \textbf{VirtualAlloc()} sur
Windows, ou \textbf{sbrk()} et \textbf{mmap()} sur UNIX. Ces fonctions ont souvent une granularité beaucoup plus grande (64 kilo octets pour \textbf{VirtualAlloc()}, 4 kilo octets
pour \textbf{mmap()}). Les allocateurs mémoires de gérer intelligemment et de manière transparente cette espace mémoire souvent trop gros pour une simple demande.

\subparagraph{Liaison avec les vulnérabilités de type Use-After-Free}
Les optimisations présentés ci-dessous, bien qu'implémenté differement selon les systèmes et bibliothèques, ont tous un point commun.
Implémenté naivement, elles augmentent certes la rapidité d'execution, mais ceci au prix d'une déterminisation quasi-compléte des zones allouées.
Une des bases des vulnérabilités de types Use-After-Free est de pouvoir allouer la même zone mémoire que celle libéré plus tôt par le programme.
Le mechanisme de cache rend cette condition rend ceci beaucoup plus facile que si les zones étaient complétement aléatoires.

\subparagraph{}
Plusieurs implémentations ont été modifiées pour palier cet effet déterministe, et de nouveaux méchanisme de libération mémoire ont également vu le jour.
MemoryProtector, sur Windows, permet par le biais des fonctions \textbf{ProtectedFree()} de ne réellement liberer la mémoire que lorsque plus aucun pointeur
résidant sur la pile du programme ne pointe à l'intérieur de la zone mémoire ciblée.\footnote{http://community.hpe.com/t5/Security-Research/Efficacy-of-MemoryProtection-against-use-after-free/ba-p/6556134\#.VuEzVEL8tpg}


\subparagraph{}
Le sujet de stage porte toutefois sur une vulnérabilité en elle-même, et ne prends pas en compte le fait que celle-ci soit expoitable selon
le système d'exploitation, l'allocateur, etc.

\section{Moteur d'execution concolique}
\myparagraph{Pourquoi de l'execution symbolique ?}
Un des défaults de l'analyse statique est l'absence de valeurs concrètes facilitant l'analyse. Même si c'est également
le but majeur de ce type d'analyse (la possibilité d'explorer tous les chemins), il est toujours agréable de conserver une valeur
associée à un identifiant ou à une zone mémoire, que ce soit sur la pile, sur le tas, ou dans d'autres segments. On peut ainsi se baser
sur ses valeurs pour évaluer la possiblité de prendre tel ou tel chemin, de réaliser des calculs, ou d'avoir tout simplement une réference en
matière de debug.
\myparagraph{Les bases de l'execution symbolique}
Ce principe d'associer une valeur non-concrète à un identifiant ou une zone mémoire est connu sous le nom d'execution symbolique. Comme son nom
l'indique, le principe est d'initialiser la valeur à un symbole lorsqu'elle est créée, et d'executer le programme en se basant sur cette valeur initiale.
Par exemple, sur x86, le registre \textbf{EAX} pourra être initialiser à la valeur \textbf{EAX\_INIT}. Ainsi une future opération \textbf{add EAX, 4} assignera
au même registre la valeur symbolique \textbf{EAX\_INIT + 4}.

\subparagraph{}
Un moteur d'execution symbolique complet contiendra également les opérations classiques en programmation, comme le dereferencement mémoire d'une valeur
symbolique, les opérations arithmétiques, etc. Concernant les opérations liés à la mémoire, la plupart des moteurs choississent de créer un espace associé à
un espace mémoire concret uniquement lorsque celui-ci est addressé en écriture. Cela permet de garder une double dimension début/taille sans pourtant autant
nécessiter une représentation exessivement volumineuse en terme de mémoire par l'outil.
\subparagraph{}
Cela peut néanmoins poser problème si le programme essaye d'accéder à une
zone non initialisé (ce qui peut être une vulnérabilité en soit pour un programme. Soit l'outil utilisé peut soit rencontrer un problème et terminer en erreur,
ou retourner une valeur nulle. Dans certains cas d'analyse, on peut néanmoins tout simplement initialisé la mémoire si celle-ci ne l'était pas déjà à une valeur concrete
par defaut ou bien à un symbole non utilisé ailleurs.

\myparagraph{Concolique: un mélange de concret et de symbolique}
Parfois, lors de l'analyse d'un programme, lorsqu'un chemin est pris par exemple, une seule valeur est possible pour tel ou tel identifiant/registre/espace mémoire.
\begin {lstlisting}[frame=single]
int main(void)
{
    int a = f();
    if (a == 0)
        g();
    else
        h();
}
\end{lstlisting}
Dans l'exemple ci-dessus, deux chemins sont possibles: l'un menant à un appel à \textbf{g()}, l'autre à \textbf{h()}.
L'analyse peut alors se séparer en deux chemins. Néanmoins, pour le chemin menant à \textbf{g()}, la valeur de l'identifiant \textbf{a}
n'a qu'une seule possibilité, la valeur 0. Certains solveurs de contraintes, dont nous parlerons dans la prochaine section, permettent d'arriver
à des conclusions comme celle ci en gardant toutes les contraintes valables pour un chemin.
\subparagraph{}
Dans des cas comme celui-ci, la valeur est directement injectée dans le moteur d'execution symbolique. Ce mélange de valeur concrete et symbolique est
appelé \textbf{execution concolique}. Cela permet de réduire le temps de calcul des assertions (la valeur correspondante étant déjà prise en compte) et de
réduire en mémoire la taille des expressions symboliques.


\section{Solveurs SMT}
\myparagraph{Contraintes associés aux chemins}
Dans un programme, une division de l'analyse en deux est toujours dû à la même structure : une condition lié à une boucle ou à une structure de type if-then-else.
Cette condition sur une valeur donnée réduit (à moins que la condition ne soit sémantiquement pas correcte initialement) le champs de valeur possible pour un identifiant.
Parfois même, la condition ou l'enchainement des conditions sur un même chemin d'execution est assez restrictive pour ne donner qu'une seule valeur possible.
Même si cela peut paraitre evident pour une analyse manuelle, il peut être délicat de résoudre ces contraintes sans aide extérieur. C'est donc le but de ce qu'on appelle : les \textbf{solveurs SMT}.
\subparagraph{}
Un \textbf{solveur SMT} (Satisfiabilité Modulo Théories) permet de résoudre un ensemble de contrainte exprimé dans la logique premier ordre. Ce type de solveur peut décider
de la solvabilité ou non de cet ensemble de contrainte. Cela permet, par exemple lors de l'analyse statique, de ne pas prendre un chemin dont les valeurs associés n'auraient
de sens d'un point de vue relationnel.
\subparagraph{}
Pour un chemin dont les contraintes sont satisfaisables selon le solveur SMT, ce même solveur peut fournir un modèle. Ce modèle represente l'ensemble des valeurs possibles
pour les variables rentrant en jeu dans les contraintes. Lorsque plusieurs valeurs sont possibles, on peut au choix et selon le but de l'analyse, sauvegarder les contraintes
en esperant de futures conditions plus restrictives, ou assigner un des ensembles de valeurs à nos identifiants symboliques.



\section{Choix d'implémentation: outils de décompilation}
\myparagraph{Critères de sélection}
Pour réaliser le sujet de stage, en dehors du language de programmation en lui-même, un ou plusieurs outils effectuant
un pré-traitement sur l'entrée était nécessaire. En effet, la solution finale prend en entrée un programme binaire et un point
d'entrée. Il n'était pas concevable d'effectuer le traitement sur du binaire, ou d'effectuer une analyse différentes selon le format
et l'architecture visée par le binaire (les instructions étant différentes).
\subparagraph{}
Un moteur d'execution symbolique est également de mise. Il représente à lui seul une grosse partie de l'analyse statique permettant de
garder en mémoire les valeurs des identifiants par rapport à leur valeur d'origine. Un solveur SMT permet également de réduire le nombre
de chemins possibles et réduit souvent drastiquement le problème d'explosion de chemins.
\subparagraph{}
Il a donc fallu chercher un ou plusieurs outils de base, pouvant effectuer la décompilation du binaire puis la transformation en une
représentation intermédiaire. Heuresement, la plupart des outils libres d'analyses binaires proposent les deux, souvent de manière transparente.
Ils proposent également leur moteur d'execution concolique associé. Le solveur SMT est la plupart du temps imposé. En effet, ce type de solveur prennent
en entrée une syntaxe et des objets particuliers, qu'il faut traduire. Ainsi la traduction de la representation intermédiaire est intriséquement lié à cette
représentation, et re-coder une traduction de chaque objet pour un solveur particulier prendrait trop de temps pour être rentable sur la durée d'un stage.
\subparagraph{}
Les outils consultés feront chacun l'objet d'un chapitre, et le choix final ainsi que les comparaisons faites seront résumés à la fin.

\myparagraph{Angr}
Angr\footnote{https://github.com/angr/pyvex} est un projet d'analyse binaire créé par le laboratoire de sécurité informatique de l'UC Santa Barbara (USA).
Il permet de charger un binaire et d'avoir une représentation binaire de ce binaire en utilisant sa propre représentation intermédiaire, \textbf{VEX}.
Il dispose également d'un moteur d'execution symbolique et est très complet dans ses fonctionnalités, permettant de récupérer pleins d'information via
une API plutôt complète. La résolution des contraintes est réalisée grâce à un module interne nommé \textbf{Claripy}
\subparagraph{}
En terme de maturité, le projet a vite atteint une autre dimension, et ses utilisateurs sont de plus en plus nombreux. Il supporte plusieurs formats
binaire (PE, ELF, IdaBin, ...) ainsi que la plupart des architectures classiques (AMD64, MIPS, ARM, ...).

\myparagraph{Miasm}
Miasm\footnote{https://github.com/cea-sec/miasm} est un projet développé principalement par des membres de l'équipe IT Security du Comité à l'Energie Atomique (CEA).
Il dispose de toutes les fonctionnalités nécessaires (moteurs d'execution, etc.). Il supporte également les architectures courantes. Le solveur de contraintes est
cependant externe, l'outil traduisant juste la représentation intermédiaire vers un langage compréhensible par le très connu Z3 de Microsoft.
\subparagraph{}
Un des désavantage de Miasm est ses fortes dépendances envers un nombre élevés de bibliothéques externes, sur des versions parfois obsolétes. La documentation est également
quasi-inexistante.

\myparagraph{Capstone}
Capston\footnote{http://www.capstone-engine.org/} est la star montante du monde de la retro-ingéniérie. En plus d'une communauté gigantesque et d'une
documentation bien construite, de nombreux projets se basent déjà sur son architecture. De plus, le projet est plutôt jeune, ce qui permets d'avoir des
dépendances recentes. La plupart des distributions ont de toute manière des paquets déjà prêt pour une installation simplifié.
\subparagraph{}
Néanmoins, aucune représentation intermédiaire n'est directement proposé par l'infrastructure de Capstone (même si plusieurs projet comme OpenReil se sont basés
dessus pour fournir une representation intermédiaire).


\myparagraph{Comparaison}

\begin{table}[H]
    \centering
    \caption{Comparatif des suites de rétro-ingéniérie/analyse choisies}
    \label{itd}
    \begin{tabular}{c|c|c|c}
                      & Angr     & Miasm   & Capstone \\
        \hline
        Graphe        & Oui      & Oui     & Non      \\
        \hline
        RI            & Oui      & Oui     & Non      \\
        \hline
        Documentation & Oui      & Non     & Oui+     \\
        \hline
        Connu         & Oui      & Oui     & Oui+     \\
        \hline
        Plateforme    & x86,MIPS & x86,ARM & A lot
    \end{tabular}
\end{table}

A l'époque à laquelle s'est déroulé cette reflexion, Angr n'avait pas vraiment été
considéré (ce qui sera d'ailleurs considéré comme une erreur plus tard lors du bilan du stage).
Miasm permettant de faire tout ce qui avait été établi lors du cahier des charges, c'est donc la suite d'outils qui fut choisie.
Même si la documentation lui fait cruellement défaut, la plupart des exemples suffisent à prendre en main (de manière basique) l'ensemble.
\subparagraph{}
Une étude plus en profondeur de l'outil a ensuite aidé à mieux affiner son usage et à utiliser des fonctions internes pour réaliser des travaux
plus finement.

\section{L'analyse statique d'Use-After-Free}
\paragraph{Graphe d'instructions}
Le graphe d'instructions d'un programme est représenté sous la forme d'un graphe orienté dont les noeuds rassemblent une ou plusieurs
instructions (intermédiaires ou liées à l'architecture). Les arcs de ce graphe relient les instructions dans leur ordre d'execution.
Un noeud peut avoir un ou plusieurs noeuds entrants, ainsi que un ou deux arcs sortants (dans le cas d'une condition par exemple).
\includegraphics[scale=1.5]{condition.png}\newline
\subparagraph{}

\section{Interprétation et critique des résultats}
\subparagraph{}
